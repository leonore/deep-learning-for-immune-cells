\chapter{Implementation} \label{sec:implementation}

\section{System diagram}

Figure \ref{fig:system} can help the reader to gain understanding how each of the materials and methods used as described in Section \ref{sec:mm} fit together.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{dissertation/figures/system_diagram.pdf}
    \caption{[TO ANNOTATE] System diagram showing how each image will be decomposed and analysed.}
    \label{fig:system}
\end{figure}

\section{Pre-processing}

Pre-processing steps are best illustrated through a diagram as shown in Figure \ref{fig:preprocessing}.

\begin{figure}[h]
    \centering
    \missingfigure[]{Figure showing pre-processing steps: raw images -> gridded images: 1. remove noise and normalise 2. obtain overlaps 3. combine images}
    \caption{Caption}
    \label{fig:preprocessing}
\end{figure}

Images were processed with a sliding window of size 192x192 as we wanted a window that was large enough to contain a few cells but also wanted to make it smaller to be able to reduce individual image dimensions further. A sliding window of 192x192 yielded 100 patches as our images were originally of size 2048x2048.

[further explanations?]

\section{Image segmentation}

Performing cell segmentation from grayscale microscope images of cells has been researched and is necessary when the cells to be studied in an image have not been separately labelled in one way or another. In our case, cell segmentation does not need to be applied as the images of T-cells and dendritic cells have been captured separately through fluorescent dyes. Instead, we are interested in using segmentation techniques to obtain the mask of each type of immune cell for the purposes of background correction and interaction quantisation, as explained in Section \ref{sec:segmentation}.

As our dataset pre-separated the type of immune cells and the cells were bright blobs on a background, we hoped that obtaining masks from each image would be straightforward. This section describes the methods we explored for this task. Both K-means and thresholding methods yielded good results and their specifics are detailed below.

\subsection{\textit{k}-means colour clustering}

k-means has been shown to perform well on image segmentation by quantising the number of colours in an image into \textit{k} clusters. Formally, k-means aims to partition data points in an array into \textit{k} sets such that the variance between points within clusters is minimised. In our case we wanted to use k-means to transform our black-and-white images of immune cells into bichrome images that we could use as masks. [The following pseudocode details the process of K-means.]

\begin{algorithm}[h]
    \DontPrintSemicolon
    \KwData{$I$, an array of pixel values making an image.\;
    $k$, the number of colours to partition the image's colour palette to.\;}
    \KwResult{A set of $k$ clusters.}
    \Begin{
        Initialise $k$ objects picked from $I$\;
        \While{clusters are still changing}
        {
           Assign each item $i$ in $I$ to the cluster with closest mean value\;
           Recompute the mean of each cluster\;
        }
    }

\caption{Pseudocode for the k-means algorithm applied to image segmentation.}
\label{alg:kmeans}
\end{algorithm}

k-means clustering is conveniently offered by multiple libraries in Python. We looked at both scikit-learn's and OpenCV's k-means. scikit-learn is a general library  for machine learning tools, while OpenCV is a more specialised library built for Computer Vision purposes. Both their k-means functions are straightforward to initialise and use. Their performance was benchmarked in order to select the best one. The table below reports times for k-means initialised with k=2, random initialisation of centroids and 10 iterations.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Initialisation} & \textbf{OpenCV} & \textbf{Scikit-learn} \\ \hline
Random                                   & \multicolumn{1}{r|}{18.9s}       & \multicolumn{1}{r|}{165s}              \\ \hline
k-means++                                & \multicolumn{1}{r|}{29.3s}       & \multicolumn{1}{r|}{139s}              \\ \hline
\end{tabular}

\caption{CPU times for OpenCV's and scikit-learn's k-means tool ran on 1,000 samples of 192x192 pixels with different methods of initialising centroids.  The computation was ran on a 2015 MacBook Pro with 2.7 GHz i5 core and 8 GB memory.}
\end{table}

As we can see OpenCV outperforms scikit-learn in all cases. OpenCV for Python is a wrapper library around the original OpenCV code built in C++, which gives it a boost in performance. OpenCV's k-means was thus selected. Initially, k-means centers were initialised randomly. However, during training and validation it was found that this method of initialisation was yielding highly different results for the intersection over union metric at every run. Hence, some speed was traded for consistency and the kmeans++ center initialisation method was picked instead.

\subsection{Thresholding}

An alternative to k-means in the case of black-and-white images is thresholding. We decided to explore this option as it could have performance improvements compared to K-means.

Thresholding refers to the process of converting a grayscale image to a binary image of pixels. Pixels above a set threshold are set to 1, and the rest of the pixels below that threshold are set to 0. Thresholding depends on pixel distribution analysis. Usually, thresholding works well for images which have different peaks of pixel values in their distribution. We can then pick the value which seems to separate out the two peaks as our threshold. However, in the case of our images we had one visible peak of pixel values. Figure \ref{fig:thresholdhist} illustrates this.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=.5\textwidth]{dissertation/figures/sample_grayscale.jpg}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=.5\textwidth]{dissertation/figures/sample_cell.jpg}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=.5\textwidth]{dissertation/figures/grayscale_histogram.png}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=.45\textwidth]{dissertation/figures/cell_histogram.png}
    \end{subfigure}
    \caption{Example images and their histogram. As we can see the grayscale image of lillies has two peaks of frequency.}
    \label{fig:thresholdhist}
\end{figure}
As such, we had to find an alternative for finding a suitable threshold. First, we selected the mean pixel value as the threshold. This yielded acceptable results, however some noisy pixels still came through the mask (see Figure \ref{fig:thresholdmean}). To fix that problem, the threshold value was set as the sum of the mean pixel value and the standard deviation. This decision was based on the hypothesis that the noise level of an image with a flat structure can be estimated from its variation. Results were satisfactory, as shown in Figure \ref{fig:thresholdstd}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[h!]{0.4\textwidth}
        \includegraphics[width=\textwidth]{dissertation/figures/mean_threshold_cell.jpg}
        \caption{Threshold: mean pixel value}
        \label{fig:thresholdmean}
    \end{subfigure}
    \begin{subfigure}[h!]{0.4\textwidth}
        \includegraphics[width=\textwidth]{dissertation/figures/mean_std_threshold_cell.jpg}
        \caption{Threshold: mean + standard deviation}
        \label{fig:thresholdstd}
    \end{subfigure}
    \caption{Segmented images according to different threshold values.}
\end{figure}

Image segmentation through thresholding is also much faster than k-means, with 1,000 images being processed in [time] with the use of Numpy arrays. Nonetheless, masks yielded by k-means are more granular. Figure \ref{fig:maskdiff} shows the difference between the masks obtained through different methods.

\begin{figure}[h]
    \centering
    \missingfigure[]{Binary image showing the absolute difference between a mask obtained with k-means and one obtained with thresholding.}
    \caption{Binary image showing the absolute difference between a mask obtained with k-means and one obtained with thresholding.}
    \label{fig:maskdiff}
\end{figure}

\subsection{UNet}

[mention UNet findings here?]

\section{Autoencoder and regression models}

\subsection{Experimental setup}

As this is a research project based on using deep learning models, a large part of the research involved an iterative process of repetitively tweaking the deep learning models, training them, and evaluating them. Immunology experiments produce a lot of imaging data, and we wanted our models to perform well on unseen data. Moreover, not all datasets contained instances of all classes. We wanted our trained model to be able to deal with such datasets too. As such, we selected a balanced dataset containing instances for all classes for training our model. Part of it was set out for validation and testing. The other two selected datasets were used purely for testing. Table \ref{table:splits} reports the number of samples in each dataset.

\begin{table}[h]
\centering
\begin{tabular}{l|r|r|r}
\rowcolor[HTML]{EFEFEF}
Dataset       & Train  & Validation & Test   \\ \hline
Three classes & 16,490 & 2,910      & 10,000 \\
DMSO          &        &            & 8,000  \\
Two classes   &        &            & 15,900
\end{tabular}
\caption{Train-test-validation splits for selected datasets. Models are only to be trained on the three-classes, balanced dataset. Validation set represents 15\% of training set.}
\label{table:splits}
\end{table}

All deep learning development was done using Keras\footnote{https://keras.io} for Python on a Tensorflow backend. Training was carried out on Google Colaboratory notebooks\footnote{https://colab.research.google.com/} to be able to accelerate computations with Google's GPUs.

While training, we used Keras's callbacks feature to monitor the model's validation loss and act accordingly. The model would reduce the learning rate by a factor of 0.2 if no improvements in the validation loss were seen over 3 epochs, and it would completely stop training if no improvements were witnessed in validation loss over 5 epochs. Training was usually ran with a batch size of 64 over 40 epochs.

\subsection{Convolutional autoencoder}

The main model to be developed was a convolutional autoencoder. The autoencoder was built for two purposes: obtaining a smaller \textit{code} representing each of the images to be fed into high-dimensional visualisation algorithms, and to be the building block for a deep regression model.

An autoencoder follows a symmetrical structure of reduction and expansion operations. The reduction operations represent the encoding part of our model, and the expansion operations represent the decoding part of our model. In order for the autoencoder input to be reduced and then reconstructed to the same number of input dimensions, we need the two sets of operations to follow the same pattern and use the same number of parameters. Figure \ref{fig:symmstruc} illustrates the symmetrical structure of an autoencoder, and shows the core layers of a convolutional autoencoder: convolution layers combined with activation functions, and pooling/upsampling layers.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{dissertation/figures/autoencoder_repeat_structure.png}
    \caption{Diagram for a typical autoencoder structure. Operations are symmetrical for the input to be reconstructed with the same dimensions at the output.}
    \label{fig:symmstruc}
\end{figure}

\textit{Convolution} refers to the process of taking the weighted sums of neighbouring values. Put another way, convolutional layers can be thought as a window sliding over an image and summing everything within that window. The specific behaviour of convolution operation is determined by the kernel it uses. Each element in the window will be multiplied by the corresponding element of the kernel. We can specify both how big the kernel to be used should be, as well as how many different kernels should be used. If we use 64 kernels, we will learn 64 features about the image, but it will also multiply our dimensions by 64. Figure \ref{} illustrates the convolution process on an image.

\textit{Activation functions} define whether or not each element in output of the convolutional layer (in this case) should be set to 'on' or 'off' i.e. whether it should be activated. The purpose of an activation function is to only keep elements of the hidden layers output if it seems to be contributing to the learning of the model.

\textit{Pooling layers} will downsample an image by passing a window over an image and selecting a set value from that window as the one to keep. There are two main types of pooling operations commonly used: average pooling and max pooling. Their counterpart are \textit{upsampling layers} ...

Throughout the development of the autoencoder, the aim was to maximise the reduction of dimensionality while maintaining a satisfactory reconstructed image. A trade-off had to be done. Size of the coded representation was mainly impacted by the number of hidden layers and size of the convolution and pooling filters. Quality of the reconstructed image could be improved with larger features being used in the convolutional layers and smaller filters.

\hfill
\subsubsection{Implementation choices}
\hfill

Knowing this, the structure of the autoencoder was tuned by trial and error and literature review of existing neural networks for other applications.

In order to avoid losing too much detail in the reconstructed image, we only used 3x3 filters in convolution layers and pooling was done by a factor of 2. Moreover, we kept the number of filters in convolution layers relatively high so as not to lose too much detail in the images. Our structure also uses decreasing filter size instead of increasing filter size. Even though increasing filter size seems to show slight improvements in reconstruction, they are minimal, and having bigger filters towards the bottleneck generates bigger coded dimensions. We also decided to replace the pooling operation in the last layer before the bottleneck by a strided convolution layer, because our experiments showed that the colours in the reconstructed image seemed slightly brighter when using striding over pooling (see Figure \ref{}).

In the hidden layers, the choice of activation function is a variant of the Rectified Linear Unit (ReLU), Parametric Rectified Linear Unit (PReLU). ReLU simply returns 0 if the weight unit in the output of a layer is less than 0, or the actual weight if it is bigger than 0. It has become popular because it has been shown to outperform the conventional sigmoid function (\cite{V. Nair and G. E. Hinton. Rectified linear units improve
restricted boltzmann machines. In ICML, pages 807–814,
2010.}, \cite{X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier
networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pages 315–323,
2011.}), as well as helping with faster training convergence (\cite{A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS,
2012.}). PReLU differs from ReLU by having a small slope for negative values which means it does not map all negative values to 0. This slope is made a learning parameter and is determined by the neural network during training. Figure \ref{relu_prelu} illustrates the difference between the two. He et al., 2015 (\cite{https://arxiv.org/pdf/1502.01852.pdf}) showed that PReLU can improve model fitting for image classification applications. Reconstruction difference using PReLU over ReLU in our network is shown in Figure \ref{}.

\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{dissertation/figures/relu_prelu.png}
    \caption{ReLU (left) vs. PReLU (right). The negative part of the PReLU function is variable and a parameter to be learned. Source: He et al., 2015}
    \label{fig:relu_prelu}
\end{figure}

The activation function at the output of the network remained the logistic sigmoid activation function, as we needed to restrict output to the [0, 1] range of our image pixels. Figure \ref{} illustrates this activation function.

Our final autoencoder structure reduced dimensions by a factor of 2 a total of 5 times, resulting in a ten-fold reduction of dimensions on the original 110,592 pixels. The final coded dimensions were of x,xxx. Figure \ref{} illustrates how the autoencoder reconstructed an input image depending on number of pooling operations. We traded-off some reconstruction quality in order to obtain a smaller coded dimension in the hope that it would help high-dimensional visualisation algorithms perform better, as well as be a better starting block for a regression model.

[training with binary cross-entropy]

The final structure of the autoencoder is shown below. Keras code for building the model is available in Appendix \ref{apdx:autoencoder}.

\subsection{Deep regression}

The regression model was built on the encoder part of the autoencoder. We wanted to use the dimensionality reduction capacities of the encoder layers to evaluate whether interaction could be quantified from an image's feature vector.

The structure of the regression model was kept simple. The encoder model was extracted from the autoencoder with the bottleneck layer flattened. This encoder model was then extended with two fully connected layers separated by a dropout layer. Dropout has shown to make models more robust and prevent overfitting (ref).

The final regression results are outputted with a fully connected layer of size 1, activated by a linear function.

The input to the regression model is 192x192x3 images accompanied with labels in the range [0, 100] representing a percentage of overlap between two cells. As such, we want the output activation function to be limited to [0, 100]. Both softplus and linear activations were candidates. The linear activation had to be used with a non-negative kernel constraint, as overlap percentage cannot be negative but the linear function normally allows for negative values. softplus output values are in the range [0, $+\infty$]. Performance in terms of training loss was similar, however the linear function performed overall better. Furthermore, softplus does not map (shown in Figure \ref{}) and is harder to differentiate compared to the linear function, which would make for slower training.

[training with mean-squared-error]

The final structure of the deep regression model is shown below. Keras code for building the model is available in Appendix \ref{apdx:regression}. 
